# Example of how to use Sharpness Aware Minimization (SAM) when training
# In particular, this can be used in combination with Allegro (https://github.com/mir-group/allegro)
# as described in:
#
#     Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization
#     Hikaru Ibayashi, Taufeq Mohammed Razakh, Liqiu Yang, Thomas Linker, Marco Olguin, Shinnosuke Hattori, Ye Luo, Rajiv K. Kalia, Aiichiro Nakano, Ken-ichi Nomura, Priya Vashishta
#     https://arxiv.org/abs/2303.08169
#
# Please additionally cite the above paper when using SAM here.

# optimizer
# SAM can be specified as the optimizer:
optimizer_name: SAM
# SAM's own parameters can be set:
optimizer_SAM_rho: 0.05
optimizer_SAM_adaptive: false
# The base optimizer must be specified:
optimizer_base_optimizer: !!python/name:torch.optim.adam.Adam ''
# And its parameters can also be set:
# (this propagates to the inner Adam optimizer)
optimizer_amsgrad: false

# -- Sections below are copied from `minimal.yaml` for a simple runnable example --

# general
root: results/aspirin
run_name: minimal-sam
seed: 123
dataset_seed: 456

# network
num_basis: 8
r_max: 4.0
l_max: 2
parity: true
num_features: 16

# data set
dataset: npz
dataset_url: http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip
dataset_file_name: ./benchmark_data/aspirin_ccsd-train.npz
key_mapping:
  z: atomic_numbers
  E: total_energy
  F: forces
  R: pos
npz_fixed_field_keys:
  - atomic_numbers

chemical_symbols:
  - H
  - O
  - C

# logging
wandb: false
verbose: debug

# training
n_train: 5
n_val: 5
batch_size: 1
validation_batch_size: 5
max_epochs: 10

# loss function
loss_coeffs: forces
